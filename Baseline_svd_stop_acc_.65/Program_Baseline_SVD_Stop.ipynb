{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd260ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\van\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\van\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\van\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\van\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\van\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\van\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36254f40-9c3a-4004-9131-58d47fa69558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Van\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libaries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1711eeef-3d2c-4749-a068-2f350e1e2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup debug for prints troubleshooting\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad17f921-0711-4537-961b-6ff6ed40c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../C50train\n",
      "['AaronPressman' 'AlanCrosby' 'AlexanderSmith' 'BenjaminKangLim'\n",
      " 'BernardHickey' 'BradDorfman' 'DarrenSchuettler' 'DavidLawder'\n",
      " 'EdnaFernandes' 'EricAuchard' 'FumikoFujisaki' 'GrahamEarnshaw'\n",
      " 'HeatherScoffield' 'JaneMacartney' 'JanLopatka' 'JimGilchrist' 'JoeOrtiz'\n",
      " 'JohnMastrini' 'JonathanBirt' 'JoWinterbottom' 'KarlPenhaul' 'KeithWeir'\n",
      " 'KevinDrawbaugh' 'KevinMorrison' 'KirstinRidley' 'KouroshKarimkhany'\n",
      " 'LydiaZajc' \"LynneO'Donnell\" 'LynnleyBrowning' 'MarcelMichelson'\n",
      " 'MarkBendeich' 'MartinWolk' 'MatthewBunce' 'MichaelConnor' 'MureDickie'\n",
      " 'NickLouth' 'PatriciaCommins' 'PeterHumphrey' 'PierreTran' 'RobinSidel'\n",
      " 'RogerFillion' 'SamuelPerry' 'SarahDavison' 'ScottHillis' 'SimonCowell'\n",
      " 'TanEeLyn' 'TheresePoletti' 'TimFarrand' 'ToddNissen' 'WilliamKazer']\n",
      "(2500,)\n",
      "(2500,)\n",
      "[50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n",
      " 50 50]\n",
      "Internet may overflowing new technology crime cyberspace still old-fashioned variety.The National Consumers League said Wednesday popular scam Internet pyramid scheme, early investors bogus fund paid deposits later investors.The league, non-profit consumer advocacy group, tracks web scams site set world wide web February called Internet Fraud Watch http://www.fraud.org.The site, collects reports directly consumers, widely praised law enforcement agencies.\"Consumers suspect scam Internet critical information,\" said Jodie Bernstein, director Federal Trade Commission's Bureau Consumer Protection. Internet Fraud Watch \"has major help FTC identifying particular scams infancy.\"In May, example, commission used Internet reports shut site run Fortuna Alliance taken $6 million, promising investors could earn $5,000 month initial deposit $250. Instead, Fortuna kept money, commission charged.Fraud reports league's site, visited 370,000 times, forwarded local, state federal authorities.The second-most-popular Internet scam, league said, sale bogus Internet services, custom designed web sites Internet access accounts.In third place crooks sell computer equipment, memory chips sound boards, net deliver significantly lower quality goods nothing all, league said.Other top scams involve business opportunities. Con artists may offer shares business franchise using unreasonable predictions misrepresentations. One popular scheme promised let consumers get rich working home.The League also announced Tuesday NationsBank donated $100,000 become sponsor Fraud Watch site.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Load in C50train located ../C50train/\n",
    "train_dir = '../C50train'\n",
    "# get name of directories, authors (these will be the labels)\n",
    "train_sub = [name for name in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, name))]\n",
    "label_lst = np.copy(train_sub)\n",
    "\n",
    "if debug:\n",
    "    print(train_dir)\n",
    "    print(label_lst)\n",
    "\n",
    "# setup the initial empty variables\n",
    "train = []\n",
    "train_v = []\n",
    "label = []\n",
    "\n",
    "# load the input data from C50train directory and process it\n",
    "\n",
    "# format will be something like this\n",
    "# ===================================\n",
    "# | Label         | Train           |\n",
    "# ===================================\n",
    "# | AaronPressman | 2537newsML.txt  |\n",
    "# ===================================\n",
    "# | AaronPressman | 14014newsML.txt |\n",
    "# ===================================\n",
    "# | ...           | ...             |\n",
    "# ===================================\n",
    "# | AlanCrosby    | 10306newsML.txt |\n",
    "# ===================================\n",
    "# | ...           | ...             |\n",
    "\n",
    "\n",
    "auth_idx = 0\n",
    "\n",
    "# setup remove filler words, aka stop words\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# go within the author directory to get list of the file names, this will be the training data\n",
    "for i in train_sub:\n",
    "    sub2_dir  = '../C50train/' + i \n",
    "    train_sub2 = [name for name in os.listdir(sub2_dir) if os.path.isfile(os.path.join(sub2_dir, name))]\n",
    "\n",
    "    #if debug:\n",
    "    #    print(sub2_dir)\n",
    "    #    print(train_sub2)\n",
    "\n",
    "    # in each author file, save the author as the label and the text as its training data\n",
    "    for j in train_sub2:\n",
    "        sub3  = '../C50train/' + i + '/' + j\n",
    "\n",
    "        with open(sub3, 'r') as file:\n",
    "            data = file.read()\n",
    "            data_no_nw = data.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "\n",
    "            # remove filler words, aka stop words\n",
    "            words = data_no_nw.split()\n",
    "            filter = [word for word in words if word.lower() not in stop]\n",
    "            filter = ' '.join(filter)\n",
    "\n",
    "            #train.append(data_no_nw)\n",
    "            train.append(filter)\n",
    "\n",
    "            # if debug:\n",
    "            #     #print(stopwords.words('english'))\n",
    "            #     print(stop)\n",
    "            #     print(words)\n",
    "            #     print(filter)\n",
    "\n",
    "\n",
    "\n",
    "        # append author index as label\n",
    "        label.append(auth_idx)\n",
    "\n",
    "    # increment author index\n",
    "    auth_idx = auth_idx + 1\n",
    "        \n",
    "        #if debug:\n",
    "        #    print(sub3)\n",
    "\n",
    "if debug:\n",
    "    print(np.shape(train))\n",
    "    print(np.shape(label))\n",
    "\n",
    "    # bin count looking at label\n",
    "    unused, idx = np.unique(label, return_counts=True)\n",
    "    #print(unused)\n",
    "    print(idx)\n",
    "\n",
    "    print(train[0])\n",
    "    print(label[0])\n",
    "    #print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, label, test_size=0.1, random_state=1)\n",
    "\n",
    "# vectorize x_train and x_test from text to matrix\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "x_train_vec = vect.fit_transform(X_train)\n",
    "x_test_vec = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5591b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph1 = False\n",
    "\n",
    "if print_graph1:\n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=2500)\n",
    "    x_svd = svd.fit_transform(x_train_vec)\n",
    "\n",
    "    # setup to calculate cumulative variance\n",
    "    cum_var = []\n",
    "    tot_var = 0\n",
    "    exp_var = svd.explained_variance_ratio_\n",
    "\n",
    "    # loop through explained varaince, sum as we go along to figure out cumulative variance\n",
    "    for ev in exp_var:\n",
    "        tot_var += ev\n",
    "        cum_var.append(tot_var)\n",
    "\n",
    "    # Plot cumulative variance\n",
    "    plt.plot(cum_var)\n",
    "    plt.title('Cumulative Variance over SVD Components')\n",
    "    plt.xlabel('SVD Components')\n",
    "    plt.ylabel('Cumulative Variance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5453605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_st = 1    # Set random state for repeatability\n",
    "n_com   = 1750 # Number of components to reduce to\n",
    "\n",
    "# Convert to array\n",
    "x_train_arr = x_train_vec.toarray()\n",
    "x_test_arr = x_test_vec.toarray()\n",
    "\n",
    "# reduce dimension with PCA\n",
    "svd = TruncatedSVD(n_components=n_com, random_state=rand_st)\n",
    "x_train_svd = svd.fit_transform(x_train_arr) \n",
    "x_test_svd = svd.transform(x_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e772bdec-4cc2-4d0b-8edf-0f3d5ce2be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.80      0.62         5\n",
      "           1       0.75      1.00      0.86         6\n",
      "           2       0.71      0.62      0.67         8\n",
      "           3       0.33      0.50      0.40         6\n",
      "           4       0.67      0.80      0.73         5\n",
      "           5       1.00      0.75      0.86         8\n",
      "           6       1.00      0.83      0.91         6\n",
      "           7       0.80      0.80      0.80         5\n",
      "           8       0.57      1.00      0.73         8\n",
      "           9       1.00      0.67      0.80         9\n",
      "          10       0.91      1.00      0.95        10\n",
      "          11       0.91      0.83      0.87        12\n",
      "          12       1.00      0.80      0.89        10\n",
      "          13       0.62      0.89      0.73         9\n",
      "          14       0.75      0.67      0.71         9\n",
      "          15       1.00      1.00      1.00         8\n",
      "          16       0.71      0.71      0.71         7\n",
      "          17       0.86      0.75      0.80         8\n",
      "          18       1.00      0.80      0.89         5\n",
      "          19       0.88      0.88      0.88         8\n",
      "          20       1.00      1.00      1.00         7\n",
      "          21       1.00      0.50      0.67         8\n",
      "          22       0.71      0.71      0.71         7\n",
      "          23       0.50      0.50      0.50         4\n",
      "          24       0.57      0.67      0.62         6\n",
      "          25       1.00      1.00      1.00         7\n",
      "          26       0.82      1.00      0.90         9\n",
      "          27       0.88      1.00      0.93         7\n",
      "          28       1.00      1.00      1.00        12\n",
      "          29       0.86      1.00      0.92         6\n",
      "          30       0.75      0.50      0.60         6\n",
      "          31       1.00      0.83      0.91        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      0.89      0.94         9\n",
      "          34       0.50      0.20      0.29        10\n",
      "          35       0.86      0.86      0.86         7\n",
      "          36       0.75      1.00      0.86         6\n",
      "          37       0.33      0.75      0.46         4\n",
      "          38       1.00      0.58      0.74        12\n",
      "          39       1.00      1.00      1.00         4\n",
      "          40       0.83      1.00      0.91         5\n",
      "          41       0.44      1.00      0.62         4\n",
      "          42       0.62      0.83      0.71         6\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.57      0.57      0.57         7\n",
      "          45       1.00      0.17      0.29        12\n",
      "          46       0.86      0.75      0.80         8\n",
      "          47       0.82      1.00      0.90         9\n",
      "          48       0.43      0.60      0.50         5\n",
      "          49       0.57      0.44      0.50         9\n",
      "\n",
      "    accuracy                           0.76       375\n",
      "   macro avg       0.77      0.77      0.75       375\n",
      "weighted avg       0.80      0.76      0.76       375\n",
      "\n",
      "LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "# use logistical regression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_svd, y_train)\n",
    "\n",
    "# predict and check accuracy\n",
    "pred = model.predict(x_test_svd)\n",
    "rep = classification_report(y_test, pred)\n",
    "\n",
    "if debug:\n",
    "    print(rep)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28313495-93f3-4639-a7c8-6ae25e2fe00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../C50test\n",
      "['AaronPressman' 'AlanCrosby' 'AlexanderSmith' 'BenjaminKangLim'\n",
      " 'BernardHickey' 'BradDorfman' 'DarrenSchuettler' 'DavidLawder'\n",
      " 'EdnaFernandes' 'EricAuchard' 'FumikoFujisaki' 'GrahamEarnshaw'\n",
      " 'HeatherScoffield' 'JaneMacartney' 'JanLopatka' 'JimGilchrist' 'JoeOrtiz'\n",
      " 'JohnMastrini' 'JonathanBirt' 'JoWinterbottom' 'KarlPenhaul' 'KeithWeir'\n",
      " 'KevinDrawbaugh' 'KevinMorrison' 'KirstinRidley' 'KouroshKarimkhany'\n",
      " 'LydiaZajc' \"LynneO'Donnell\" 'LynnleyBrowning' 'MarcelMichelson'\n",
      " 'MarkBendeich' 'MartinWolk' 'MatthewBunce' 'MichaelConnor' 'MureDickie'\n",
      " 'NickLouth' 'PatriciaCommins' 'PeterHumphrey' 'PierreTran' 'RobinSidel'\n",
      " 'RogerFillion' 'SamuelPerry' 'SarahDavison' 'ScottHillis' 'SimonCowell'\n",
      " 'TanEeLyn' 'TheresePoletti' 'TimFarrand' 'ToddNissen' 'WilliamKazer']\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "# Load in C50test located ../C50test/\n",
    "test_dir = '../C50test'\n",
    "# get name of directories, authors (these will be the labels)\n",
    "test_sub = [name for name in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, name))]\n",
    "test_lst = np.copy(train_sub)\n",
    "\n",
    "if debug:\n",
    "    print(test_dir)\n",
    "    print(test_lst)\n",
    "\n",
    "# setup the initial empty variables\n",
    "test       = []\n",
    "test_label = []\n",
    "\n",
    "# load the input data from C50test directory and process it\n",
    "\n",
    "auth_idx = 0\n",
    "\n",
    "# go within the author directory to get list of the file names, this will be the training data\n",
    "for i in train_sub:\n",
    "    sub2_dir  = '../C50test/' + i \n",
    "    test_sub2 = [name for name in os.listdir(sub2_dir) if os.path.isfile(os.path.join(sub2_dir, name))]\n",
    "\n",
    "    #if debug:\n",
    "    #    print(sub2_dir)\n",
    "    #    print(train_sub2)\n",
    "        \n",
    "    # in each author file, save the text as its test data\n",
    "    for j in test_sub2:\n",
    "        sub3  = '../C50test/' + i + '/' + j\n",
    "\n",
    "        with open(sub3, 'r') as file:\n",
    "            data = file.read()\n",
    "            data_no_nw = data.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "            # remove filler words, aka stop words\n",
    "            words = data_no_nw.split()\n",
    "            filter = [word for word in words if word.lower() not in stop]\n",
    "            filter = ' '.join(filter)\n",
    "\n",
    "            #train.append(data_no_nw)\n",
    "            test.append(filter) \n",
    "        \n",
    "        test_label.append(auth_idx)\n",
    "\n",
    "    auth_idx = auth_idx + 1\n",
    "\n",
    "if debug:\n",
    "    print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "771a41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "new_test_vec = vect.transform(test)\n",
    "\n",
    "new_test_arr = new_test_vec.toarray()\n",
    "new_test_svd = svd.transform(new_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2afa7db-fa43-4daa-9ae4-715cb3768b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 41  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0 44  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 41  0  0\n",
      "  5  0  1  1  1  1 17  1 14 17  1 21 17 17  1 14 14  1  1  1  1  1  1  1\n",
      "  1  1  1  1 17 17 17 17 17 17  1  1  1  1 14  1 14  1  1  1  1 17 17  1\n",
      "  1  1  1  1  2 16  2 16 16 16 16 16 16 16  2 16  2 16 16 16 16 16 16 16\n",
      " 16 16  2 16 16 16 16  2 44  2 44  2 16 16  0  2 16  2 16 16 16 16 16 16\n",
      " 16 18  2  2 44  2 13 37  3 49 11 27 11 49  3 13  3  3 43  3 37 43 13  3\n",
      " 13  3 49 13 13  3 37  3  3 49 49 49 43 43 13 13 34 20 13 34 34 34 34 34\n",
      "  3 13 49 34  3 49 43 43  4  4  4  4  4  4  4  4  4 23  4  5  5 23 23 30\n",
      " 23  4 30 23 30 23 23 23  0  4  4  0  4 23  4 23  4  4 23 23 23 23 23  4\n",
      "  4  4  4 23 23 23 23 23 23 30  5  5  5  5  5  5 41  5  5  5  5  9  5  5\n",
      " 39  5  5 22  5  5  5  5 22  5  5  5  5  9  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5 35  5  5  5  5  5 12  6  6  6  6 12  6  6  6 12  6 12\n",
      "  6 12 12 12 12 12 12 12  6 12 12 12 12 12 12 12  6 12 12 12 12  6  6 12\n",
      " 12  6 12 12 12 12 12 12 12 12 12 12 12 12 48  7  7 48 36 36  7  5  5 48\n",
      " 48 48 33 48  7 48 39 48  7 48 41 48 48 48  7 48 48 48 48 48 48 48 48  7\n",
      " 48 48 43 48  7  7  7 48 48 48  5  7  7 48  7  7  8  8 30  8  8  8 39 39\n",
      " 39 39 47  8  8 19 19 19  8  8 48 30  8 18 47 47 48 48  8 19 48 48 48 48\n",
      " 48  8  8  8 18  8  8  8  8  8  8 47 39 39  8  8  8  8 22  9  9  9  9  9\n",
      "  9  9 22  9 46 46  9  9  5 40 46 25 41 41  9  9  9  9  9 41  9  9 46 46\n",
      "  9 41 40 40 40 46  9 35 35 22 35 35 35 35 35 35 35 35 35 41 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11\n",
      " 11 49 11 11 11 15 11 11 11 43 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 13 11 13 43 11 11\n",
      " 12 12 12 12 12 12 12 12  6  6 12 12 12 12  6 12 12  6  6  6  6 12  6 12\n",
      " 12  6  6 17 17 17  6 12  6 13  6  6 43 43 43 13 13 43 43 29 17 20  6  6\n",
      " 12  6 11 43 43 43 34 34 43 34 43  3 37 13 49  3 13 11  3 34 49 34 13 43\n",
      "  3 34 34 43 43 34 43 34 34 34 43 13 43 43 43 43 43 43  3 43 43 43 43 43\n",
      " 43 43 43  3 14 14 14 17 14 17 17 17 17 17 17 14  1 17 28 17 28 17 17 17\n",
      " 17 14 17 14 14 28 17 14 17 17 14 14 14 14 28 17 17 17 17 17 17 17 17 17\n",
      " 14  1  1 14 17 14 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16  8 16 16 16 16  2 44 16 23\n",
      " 23 44 44 16 16 16 16 16 16 16 16 16 16 16 16 16  2 16 16  2 16 16 16 16\n",
      " 44 44 44 44 44 16 16 16 16 44 17 17 17 17 17 17 17 17 14 17 17 14 17 17\n",
      " 14 14 14 17 17 17 17  1 17 17 17 14 17 14 17 14 14 28 17 17 17 17 17 17\n",
      " 17 17 17 17 17 17 17 17 17 14 14 14 18 18 18 19 18 18 18 18 18 28 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 47 47 18  8 18 44 44 47 47 18 18 18\n",
      "  8 18 18 18 18 47 18 18 18 18  8 18 18 18 19 19 19 19 19 19 19 19 19 19\n",
      " 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19  5 47\n",
      " 19 19 19 19 19 19 19 19 19 19 19 19 19 18 19]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87        50\n",
      "           1       0.89      0.64      0.74        50\n",
      "           2       0.65      0.26      0.37        50\n",
      "           3       0.40      0.24      0.30        50\n",
      "           4       0.62      0.42      0.50        50\n",
      "           5       0.62      0.86      0.72        50\n",
      "           6       0.33      0.28      0.30        50\n",
      "           7       0.58      0.28      0.38        50\n",
      "           8       0.77      0.48      0.59        50\n",
      "           9       0.40      0.38      0.39        50\n",
      "          10       0.91      1.00      0.95        50\n",
      "          11       0.77      0.88      0.82        50\n",
      "          12       0.33      0.38      0.35        50\n",
      "          13       0.12      0.08      0.10        50\n",
      "          14       0.48      0.32      0.39        50\n",
      "          15       0.89      1.00      0.94        50\n",
      "          16       0.49      0.70      0.57        50\n",
      "          17       0.46      0.72      0.56        50\n",
      "          18       0.72      0.76      0.74        50\n",
      "          19       0.87      0.94      0.90        50\n",
      "          20       0.91      1.00      0.95        50\n",
      "          21       0.78      0.80      0.79        50\n",
      "          22       0.60      0.78      0.68        50\n",
      "          23       0.45      0.66      0.53        50\n",
      "          24       0.89      0.50      0.64        50\n",
      "          25       0.77      0.66      0.71        50\n",
      "          26       0.97      0.68      0.80        50\n",
      "          27       0.89      0.80      0.84        50\n",
      "          28       0.84      0.98      0.91        50\n",
      "          29       0.66      0.76      0.70        50\n",
      "          30       0.72      0.58      0.64        50\n",
      "          31       0.79      0.46      0.58        50\n",
      "          32       0.98      0.90      0.94        50\n",
      "          33       0.84      0.86      0.85        50\n",
      "          34       0.23      0.30      0.26        50\n",
      "          35       0.70      0.84      0.76        50\n",
      "          36       0.78      0.62      0.69        50\n",
      "          37       0.41      0.84      0.55        50\n",
      "          38       0.81      0.52      0.63        50\n",
      "          39       0.77      0.80      0.78        50\n",
      "          40       0.88      0.76      0.82        50\n",
      "          41       0.51      0.72      0.60        50\n",
      "          42       0.57      0.50      0.53        50\n",
      "          43       0.24      0.38      0.30        50\n",
      "          44       0.64      0.88      0.74        50\n",
      "          45       0.69      0.18      0.29        50\n",
      "          46       0.81      0.84      0.82        50\n",
      "          47       0.74      0.78      0.76        50\n",
      "          48       0.38      0.52      0.44        50\n",
      "          49       0.32      0.26      0.29        50\n",
      "\n",
      "    accuracy                           0.63      2500\n",
      "   macro avg       0.65      0.63      0.63      2500\n",
      "weighted avg       0.65      0.63      0.63      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# predict with model\n",
    "#new_pred = model.predict(new_test_vec)\n",
    "new_pred = model.predict(new_test_svd)\n",
    "\n",
    "# get report to print out accuracy\n",
    "rep = classification_report(test_label, new_pred)\n",
    "\n",
    "if debug:\n",
    "    print(new_pred[0:999])\n",
    "    print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
